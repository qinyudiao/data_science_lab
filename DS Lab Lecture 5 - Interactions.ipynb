{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets, linear_model\n",
    "import pandas as pd\n",
    "from pandas import DataFrame, Series\n",
    "import seaborn as sns\n",
    "import math\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "sns.set(style='ticks', palette='Set2')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is the notebook we saw in Lecture 5. There are a few things we have not discussed in lecture, specifically, R^2. You can read up on this, or skip it... the main point for now is the idea of visualizing and then adding an interaction term. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The dataset for this example is from the website for the book by Gelman and Hill, on Multilevel Regression. You can download it from there. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kidiq = pd.read_stata('kidiq.dta')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling the relationship between the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have output typically denoted by: $Y_i$, $i=1,...,n$, and we denote the the input by $X_{ij}$, $i=1,...,n$, $j=1,...,p$. $X_{ij}$ is the $i^{th}$ observation of the $j^{th}$ covariate. \n",
    "\n",
    "We encode all $n$ values of $Y_i$ in the vector $Y$, and all values of $X_{ij}$ in the matrix $X$.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jargon:\n",
    "    \n",
    "    Y: response variables, target variables, dependent variables, outcomes.\n",
    "    X: features, covariates, explanatory variables, independent variables, design matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What should we do? How are X and Y related? How should we search for relationships?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kidiq.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the \"Y\" and what is the \"X\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some notes:\n",
    "\n",
    "    Some variables are continuous (or close): kid_score, mom_iq, mom_age\n",
    "    Other variables are categorical: mom_hs, mom_work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal: how are X and Y related?\n",
    "$$\n",
    "Y_i \\approx f(X_i)\n",
    "$$\n",
    "(What are the relevant dimensions?)\n",
    "\n",
    "What is such a model good for?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models: \n",
    "\n",
    "    * Prediction\n",
    "    * Correlations/Relationships\n",
    "    * Causality(?)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(kidiq)\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that kid_score and mom_iq may have some correlation. Let's see these together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(\"mom_iq\",\"kid_score\",kidiq)\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use other (even categorical) variables. For example, let's use mom_hs and kid_score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(\"mom_hs\",\"kid_score\",kidiq)\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some Pandas/Python practice: we extract the numpy arrays from the dataframe, and use linear_model from scikit-learn to perform the same fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kidiq_X = kidiq.as_matrix([\"mom_iq\"])\n",
    "kidiq_Y = kidiq.as_matrix([\"kid_score\"])\n",
    "lr = linear_model.LinearRegression()\n",
    "lr.fit(kidiq_X,kidiq_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the coefficients we found.\n",
    "print (lr.coef_, lr.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the interpretation of these numbers? When we plot this, 25.8 is the intercept, and 0.61 is the slope. What does that mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And we can plot the fit again:\n",
    "plt.scatter(kidiq_X, kidiq_Y,  color='black')\n",
    "plt.plot(kidiq_X, lr.predict(kidiq_X), color='blue',\n",
    "         linewidth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's repeat, but now solving a multiple regression, with control variables \n",
    "# (or independent variables) mom_iq and also the categorical variable mom_hs\n",
    "kidiq_X = kidiq.as_matrix([\"mom_iq\",\"mom_hs\"])\n",
    "kidiq_Y = kidiq.as_matrix([\"kid_score\"])\n",
    "lr = linear_model.LinearRegression()\n",
    "lr.fit(kidiq_X,kidiq_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the coefficients we found.\n",
    "print (lr.coef_, lr.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the coefficient multiplying \"mom_iq\" has changed -- it is now just 0.56. What does that mean about the new covariates? What does it mean about our original conclusion and the assumptions behind that?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "We can compute this same solution manually, using the formula:\n",
    "$$\n",
    "\\hat{\\beta} = (X^{\\top}X)^{-1}X^{\\top}y\n",
    "$$\n",
    "We have to make sure that we allow for the constant term by appending a column of all ones to X. Then we can apply the above formula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.column_stack((kidiq_X,np.ones(kidiq_Y.shape[0])))\n",
    "np.dot(np.dot(np.linalg.inv(np.dot(X.transpose(),X)),X.transpose()),kidiq_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing this explicit formula for $\\hat{\\beta}$ helps us start answering the question: How much do we trust these coefficients? We can compute what is called the standard error of the coefficients of $\\hat{\\beta}$. And writing it explicitly as above, helps with this.\n",
    "\n",
    "Supose the \"truth\" (let us imagine/pretend for a moment that the model is the truth...) is\n",
    "$$\n",
    "y = X \\beta + w,\n",
    "$$\n",
    "where $w \\sim N(0,\\sigma^2)$. Then plugging this in to our formulat for $\\hat{\\beta}$ above, we have:\n",
    "\n",
    "$$\n",
    "\\hat{\\beta}=(X^{\\top}X)^{-1}X^{\\top}y = (X^{\\top}X)^{-1}X^{\\top}(X \\beta + w) = \\beta + (X^{\\top}X)^{-1}X^{\\top}w.\n",
    "$$\n",
    "\n",
    "For one thing, this tells us that $\\hat{\\beta}$ is unbiased. \n",
    "\n",
    "Beyond this, it tells us what its distribution is. This is somewhat of a subtle point. What do we mean that $\\hat{\\beta}$ has a distribution, when it is just a deterministic function of the data??\n",
    "\n",
    "Before we see the data, we can think of $y$ as being random, because it is a function of $\\beta$, but also of the additive noise. Therefore, before we see $X$ and $y$, then $\\hat{\\beta}$ is also random. It is important to think of it this way. It's like saying: \"if we collect more data from a \"statistically identical\" population, what kind of variation would we expect to see in our solution, $\\hat{\\beta}$? This is another way to get to the answer of \"how much do we trust our solution?\"\n",
    "\n",
    "Thus, since we know that\n",
    "\n",
    "$$\n",
    "\\hat{\\beta} = \\beta + (X^{\\top}X)^{-1}X^{\\top}w,\n",
    "$$\n",
    "\n",
    "we can compute the variance, and hence the standard deviation, of $\\hat{\\beta}_1,\\dots,\\hat{\\beta}_n$. This quantity is called the standard error. Anything within about 2 standard deviations would be \"reasonable\" or \"plausible\". Note that different coefficients of $\\hat{\\beta}$ can have different standard errors.\n",
    "\n",
    "Let us compute this for the example above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = np.linalg.inv(np.dot(X.transpose(),X))\n",
    "sigma = np.sqrt(sum((kidiq_Y-np.mean(kidiq_Y))**2)/(kidiq_Y.shape[0]-1))\n",
    "std_error = np.sqrt(np.diagonal(V))*sigma\n",
    "# The standard errors:\n",
    "print (\"The standard errors are\", std_error)\n",
    "print (\"Hence all our computed values are significant, in that they do not change sign within two standard deviations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Residuals and $R^2$-Values\n",
    "\n",
    "Recall our discussion about how much statistical noise, or fluctuations, we should expect. This relates to the quality of the fit. What can we say about the errors of our estimate?\n",
    "\n",
    "To begin answering this question, we have to consider the vector of residuals. \n",
    "\n",
    "Let $\\hat{Y}$ be the predicted values given by $\\hat{\\beta}$, that is, $\\hat{Y} = X \\hat{\\beta}$. Then the residuals are given by the difference between the true output ($Y$) and the fitted (or predicted) output ($\\hat{Y}$):\n",
    "$$\n",
    "\\hat{r} = Y - \\hat{Y} = Y - X \\hat{\\beta}\n",
    "$$\n",
    "# Exercise (Optional): show that the residuals sum to zero.\n",
    "\n",
    "In other words: \n",
    "$$\n",
    "\\bar{Y} = \\frac{1}{n} \\sum Y_i = \\frac{1}{n} \\sum \\hat{Y}_i = \\hat{\\bar{Y}}\n",
    "$$\n",
    "\n",
    "The R-Value is a measure of goodness of fit. It tries to capture how much of the variation in $Y$ is explained by $\\hat{Y}$.\n",
    "# Show that $$ \\|Y\\|^2 - n \\bar{Y}^2 = \\|\\hat{r}\\|^2 + \\|\\hat{Y}\\|^2 - n \\hat{\\bar{Y}}^2.$$\n",
    "\n",
    "The point of this exercise is that it suggests that a measure of the goodness of fit, is the variance of $\\hat{Y}$ compared to the variance of $Y$.\n",
    "\n",
    "This is the definition of the $R^2$ criterion:\n",
    "$$\n",
    "R^2 = \\frac{\\sum (\\hat{Y}_i - \\hat{\\bar{Y}})^2}{\\sum (Y_i - \\bar{Y})^2}.\n",
    "$$\n",
    "Note that $R^2 \\leq 1$. Bigger is interpreted as better. $R^2=1$ means that $\\hat{Y}$ \"explains\" all the variance in $Y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's compute R^2 when we fit to just mom_iq\n",
    "# Let's repeat, but now solving a multiple regression, with control variables \n",
    "# (or independent variables) mom_iq and also the categorical variable mom_hs\n",
    "kidiq_X = kidiq.as_matrix([\"mom_iq\"])\n",
    "kidiq_Y = kidiq.as_matrix([\"kid_score\"])\n",
    "lr = linear_model.LinearRegression()\n",
    "lr.fit(kidiq_X,kidiq_Y)\n",
    "lr.score(kidiq_X,kidiq_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and when we fit to mom_iq and to mom_hs\n",
    "kidiq_X = kidiq.as_matrix([\"mom_iq\",\"mom_hs\"])\n",
    "kidiq_Y = kidiq.as_matrix([\"kid_score\"])\n",
    "lr = linear_model.LinearRegression()\n",
    "lr.fit(kidiq_X,kidiq_Y)\n",
    "lr.score(kidiq_X,kidiq_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And now let's try fitting to everything\n",
    "kidiq_X = kidiq.as_matrix([\"mom_iq\",\"mom_hs\",\"mom_work\",\"mom_age\"])\n",
    "kidiq_Y = kidiq.as_matrix([\"kid_score\"])\n",
    "lr = linear_model.LinearRegression()\n",
    "lr.fit(kidiq_X,kidiq_Y)\n",
    "lr.score(kidiq_X,kidiq_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again the R^2 value is not a final test. It is merely another interpretation of the goodness of fit, that can help provide some guidance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Back to fitting mom_iq and mom_hs. Is this a good idea? What does the regression mean?\n",
    "# and when we fit to mom_iq and to mom_hs\n",
    "kidiq_X = kidiq.as_matrix([\"mom_iq\",\"mom_hs\"])\n",
    "kidiq_Y = kidiq.as_matrix([\"kid_score\"])\n",
    "lr = linear_model.LinearRegression()\n",
    "lr.fit(kidiq_X,kidiq_Y)\n",
    "print (lr.coef_, lr.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that we are fitting a model of the form: \n",
    "\n",
    "$${\\rm kid.score} = 25.7 + 0.56 * {\\rm mom\\_iq} + 6 * {\\rm mom\\_hs}$$\n",
    "\n",
    "Note that this means: the coefficient of mom_iq is (forced to be) the same within each group. That is, this model says (by design) that the association of mom_iq on kid_score is the same, regardless of mom_hs.\n",
    "\n",
    "Do the data support this?\n",
    "\n",
    "We can plot the resulting lines for each subgroup: mom_hs = 0 and mom_hs = 1. That is, we can plot the conditional regression inside each subgroup. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kidiq_hs = kidiq[kidiq.mom_hs==1]\n",
    "kidiq_nohs = kidiq[kidiq.mom_hs==0]\n",
    "X_hs = kidiq_hs.as_matrix([\"mom_iq\"])\n",
    "Y_hs = kidiq_hs.as_matrix([\"kid_score\"])\n",
    "X_nohs = kidiq_nohs.as_matrix([\"mom_iq\"])\n",
    "Y_nohs = kidiq_nohs.as_matrix([\"kid_score\"])\n",
    "# And we can plot the fit again:\n",
    "plt.scatter(X_hs, Y_hs,  color='black')\n",
    "plt.scatter(X_nohs, Y_nohs,  color='blue')\n",
    "plt.plot(X_hs, 25.7+0.56*X_hs + 6, color='black',\n",
    "         linewidth=2)\n",
    "plt.plot(X_nohs, 25.7+0.56*X_nohs, color='blue',\n",
    "         linewidth=2)\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Include an interaction term\n",
    "Specifically: load the kidiq data set into a data frame, figure out how to add an interaction term, and then solve the resulting linear regression now with three features (three covariates), namely, mom_iq, mom_hs, and mom_hs * mom_iq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
